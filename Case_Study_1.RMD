---
title: "Case_Study_1"
author: "Team Project - Babatunde John Olanipekun, Yvan Sojdehei and Mingyang Nick YU"
date: "9/30/2020"
purpose: "This is a team case study aiming at exploring/analyzing dataset (Beers.csv and Breweries.csv) provided by CEO and CFO of Budweiser, answer specific questions and beyond"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Beer and Breweries Exploration
Introduction: This is an exploratory data analysis for the data set between Beers.csv and Breweries.csv for CEO and CFO of Budweiser

### Loading the datasets
Doing intial inspection of data through various methods below.
```{r}
# dataset should be in the same folder of this RMD file
Beers = read.csv("/Users/mingyang/Desktop/SMU/DoingDS_Fall2020/MSDS6306-Case-Study1/Beers.csv",header = TRUE) #loading beers dataset
Breweries = read.csv("/Users/mingyang/Desktop/SMU/DoingDS_Fall2020/MSDS6306-Case-Study1/Breweries.csv",header = TRUE) #loading breweries dataset
#below this line is for self analyzation
#summary(Beers)
#str(Beers)
##Beers$IBU
#summary(Breweries)
#str(Breweries)
#Above this line is for self analyzing can be deleted later
#Turn Breweries State column into a factor
Breweries$State = as.factor(Breweries$State)
```

### How many breweries are present in each state?
- As we can see by the plot below, each state's breweries count is displayed in a bar chart 
- To make it easier to see which state has the most breweries, we assorted the breweries count by descending order
```{r}
library(tidyverse) 
library(ggplot2)
num_Breweries_by_state = Breweries %>% group_by(State) %>%
  summarise(count=n())
num_Breweries_by_state
#As we can see the number of breweries per state is in the list below, to see this better we will use a plot to show results
ggplot(data=num_Breweries_by_state)+
  geom_bar(mapping=aes(x=State,y=count,fill=State),stat="identity") +
  ggtitle("Breweries count by state")+xlab("State")+ylab("Count of Breweries")
#To see this by descend of count
ggplot(data=num_Breweries_by_state)+
  geom_bar(mapping=aes(x=reorder(State,-count),y=count,fill=State),stat="identity") +
  ggtitle("Breweries count by state")+xlab("State")+ylab("Count of Breweries")
```

### Merge beer data with the breweries data. Print the first 6 observations and the last six observations to check the merged file.  
```{r}
Beers = Beers %>% rename(Brew_ID= Brewery_id)
Beers.with.Breweries = left_join(Beers,Breweries, by = "Brew_ID")
Beers.with.Breweries = Beers.with.Breweries %>% rename(Beer_Name= Name.x)
Beers.with.Breweries = Beers.with.Breweries %>% rename(Brew_Name= Name.y)
head(Beers.with.Breweries,6)
```


### Address the missing values in each column
- There are 62 missing values on ABV column, and there are 1005 missing values on IBU column
- Since 1005/2410 is too big of a percentage of missing value for column IBU, I decided to use Predictive mean matching method(widely used statistical imputation method for missing values, first proposed by Donald B. Rubin in 1986 and R. J. A. Little in 1988) to fill the missing data.
- After generating and filling in the missing data, through density plot below, we can see the generated filled in value matches the original density plot. Thus minimize any bias from the researcher of arbitrarily filling in value or overeasily using the overall mean for all missing value
```{r}
summary(Beers.with.Breweries)
library(mice) #Load mice library to analyze the pattern of missing data
md.pattern(Beers.with.Breweries)
# Since there is large amont of data missing in IBM column 
#Try to impute the missing data with Predictive mean Matching method
tempData <- mice(Beers.with.Breweries,m=5,maxit=50,meth='pmm',seed=20)
#summary(tempData)
# completed dataset after adding in generated predictive values
completedData <- complete(tempData,1)
#head(completedData)
# Density plot original vs imputed dataset
densityplot(tempData)
#Note: idea used above to impute data is from link below:
#https://datascienceplus.com/imputing-missing-data-with-r-mice-package/
```
### Compute the median alcohol content and international bitterness unit for each state. Plot a bar chart to compare
- First plotted Median alcohol content by state, one with modified data set, and the other one with non-modified data set
- Then plotted Median international bitterness unit for each state, both on the modified data set and non-modified
- We can see between the modified data and non-modified data provides very different answers. If possible, going back to try to complete the missing data will make the graphics more accurate. But this is a random estimate of the actual value
```{r}
#Compute and display Median of ABV and IBU by state:
median = completedData %>% group_by(State) %>%
  summarize(median_ABV=median(ABV),median_IBU=median(IBU))
median
#Draw Bar Charts to compare
#First plot median of alcohol content using modified data
median %>% ggplot()+
  geom_bar(mapping=aes(x=reorder(State,-median_ABV),y=median_ABV,fill=State),stat="identity") +
  ggtitle("Median Alcohol content by State on modified dataset")+xlab("State")+ylab("Alcohol Content Percentage")
#Below is result of using complete data set with missing data to plot median of alcohol content
Beers.with.Breweries %>% group_by(State) %>%
  summarize(median_ABV=median(ABV),median_IBU=median(IBU))%>% ggplot()+
  geom_bar(mapping=aes(x=reorder(State,-median_ABV),y=median_ABV,fill=State),stat="identity")+
  ggtitle("Median Alcohol content by State on non-Modified dataset")+xlab("State")+ylab("Alcohol Content Percentage")
#Below is result of plotting median international bitterness unit for each state on modified data set
median %>% ggplot()+
  geom_bar(mapping=aes(x=reorder(State,-median_IBU),y=median_IBU,fill=State),stat="identity") +
  ggtitle("Median International Bitterness Unit by State on modified dataset")+xlab("State")+ylab("International Bitterness Unit")
#Below is result of using complete data set with missing data to plot median of alcohol content
Beers.with.Breweries %>% group_by(State) %>%
  summarize(median_ABV=median(ABV),median_IBU=median(IBU))%>% ggplot()+
  geom_bar(mapping=aes(x=reorder(State,-median_IBU),y=median_IBU,fill=State),stat="identity") +
  ggtitle("Median International Bitterness Unit by State on non-modified dataset")+xlab("State")+ylab("International Bitterness Unit")

```

### Exploring which state has the maximum alcoholic (ABV) beer and which state has the most bitter (IBU) beer
- As we can see below, Colorado 'CO' has the maximum alcoholic(ABV) beer, it is (0.128)
- This above result remain unchanged with the original data set
- As we can see, Oregon and Michigan have the most bitter beer
- Only Oregon has the most bitter beer in the unchanged data set. It means the Michigan bee bitterness value was imputed
```{r}
# Discover which state has the maximum alcoholic beer
head(completedData %>%
  arrange(desc(ABV)) %>% 
  select(State,ABV,Beer_Name))
# Discover with un-changed data set
head(Beers.with.Breweries %>%
  arrange(desc(ABV)) %>% 
  select(State,ABV,Beer_Name))

# Discover which state has the most bitter (IBU) beer
head(completedData %>%
  arrange(desc(IBU)) %>% 
  select(State,IBU,Beer_Name))

# Discover with un-changed data set
head(Beers.with.Breweries %>%
  arrange(desc(IBU)) %>% 
  select(State,IBU,Beer_Name))
  
```

### Comment on the summary statistics and distribution of the ABV variable.
- **For the adjusted data set, we have**
- Minimum ABV value: 0.001 (minimum value of the group, CEO and CFO might be interested)
- First quartile ABV: 0.05
- Median of ABV: 0.056 (center among the entire group, CEO and CFO might be interested)
- Mean of ABV: 0.05975 (Average of the entire group, CEO and CFO might be interested)
- Third Quartile ABV: 0.067
- Maximum ABV: 0.128 (maximum value of the group, CEO and CFO might be interested)
- **Comparing the adjusted Data and Unadjusted data, Statistics doesn't change much at all since it is imputed by Predictive mean matching method**

```{r}
# Summary of adjusted Data
summary(completedData)
# Summary of unadjusted Data
summary(Beers.with.Breweries)
```
### Is there an apparent relationship between the bitterness of the beer and its alcoholic content?
- **We can see from the scatter plot, there seem to be some sort of relationship between the two**
- As ABV value increases, IBU seem to also increase
- **This relationship seem to persist when we compare to the unmodified data**
```{r}
#first explore modified data
completedData %>% select(ABV, IBU, State) %>%
  ggplot(aes(x=ABV,y=IBU)) +
  geom_point(position="jitter") +ggtitle("IBU vs. ABV -- modified data")
#next explore unmodified data
Beers.with.Breweries %>% select(ABV, IBU, State) %>%
  ggplot(aes(x=ABV,y=IBU)) +
  geom_point(position="jitter") +ggtitle("IBU vs. ABV -- unmodified data")
  
```

### Budweiser would also like to investigate the difference with respect to IBU and ABV between IPAs (India Pale Ales) and other types of Ale (any beer with “Ale” in its name other than IPA).
- There are total of 618 Beer name has Ale in it
- 41 Beer name has India Pale Ales in it
- 577 are other types of Ale
- By using KNN model to try classifying India Pale Ales against other Ales
- Initially used k=5 (five neighbors to compare): really great Accuracy rate 0.9355, however very low sensitivity rate with the model (correctly predicting India Pale Ale), great Specificity rate (correctly predicting other types of Ales)
- Next we tried different random seed number and see if this is due to chance: by trying another randomn seed model, got accuracy rate to 0.9597, increased Sensitivity to 0.33, and Super great Specificity: 0.99 (99% chance of predicting other types of Ales)
- Then we explored what is the best k value to give the most accuracy, after exploring, it is giving k>30, however, this reduced the Sensitivity (successfully predicting India Pale Ales) to almost minimal. It leads us to explore what k gives best Sensitivity rate(try to improve our model sensitivity rate)?
- After running such test, it suggest a low k number provides better Sensitivity
- In this scenario, I consider k=3 might be the best compromise to balance the model
- By using k=3, seed=101, we got Accuracy rate=0.92, while Sensitivity is around 0.33(successfully predicting India Pale Ales), Specificity=0.95(Successfully predict other Ales) 
- Getting an average result of one hundred random generations from KNN model with k=3, Average Accuracy = 0.93, Average Sensitivity = 0.24, Average Specificity = 0.98
- **Conclusion: Even we have a great accuracy rate of predicting with our KNN model using IBU and ABV,we have a poor sensitivity rate to predict India Pale Ales correctly, this maybe due to the fact that the other Ales groups covers majority of the data set, and thus there is a high chance for any Ale to be categorized to other Ale group based on IBU and adjusted ABV alone.**
- Next try Naive Bayes Models to use probabilities to estimate different Ales based on IBU and ABV
- After trying Naive Bayes Model, it can give an improved Sensitivity rate with the right random generator
- By getting the average of one hundred random generations from NB model, Average Accuracy = 0.90, Average Sensitivity= 0.15, Average Specificity = 0.95
- **Conclusion with NB model - not as good as KNN with k=3 **
```{r}
#In order to investigate the difference respect to IBU and ABV, first extract all name with Ales
#getting all bear name with ale in it (ignore the case factor)
all_ales = completedData %>% filter(str_detect(completedData$Beer_Name,regex("Ale",ignore.case=TRUE)))
india_pale_ales = all_ales %>% 
  filter(str_detect(all_ales$Beer_Name,regex("India Pale Ale",ignore.case=TRUE)))
other_ales = all_ales %>% 
  filter(!str_detect(all_ales$Beer_Name,regex("India Pale Ale",ignore.case=TRUE)))
# in order to effectively use KNN model, I decided to standardize percentage scale of ABV, so the effect of distance from ABV and IBU are roughly the same. I choose to time each ABV value by 1000 to accomplish this standarization
all_ales$ABV = all_ales$ABV * 1000
# Change other ales name to other ales in order to use KNN model to test whether we can use IBU and ABV to distinguish IPAs from others
all_ales = all_ales %>% 
  mutate(India.Pale.Ale.Or.Else = ifelse(str_detect(Beer_Name,regex("India Pale Ale",ignore.case=TRUE)),"India Pale Ale","Other Ale"))
#all_ales$India.Pale.Ale.Or.Else
# Start KNN test to see how good it is to use ABV and IBU to distinguish the Ales
library(class)
library(caret)
library(e1071)
all_ales$India.Pale.Ale.Or.Else = as.factor(all_ales$India.Pale.Ale.Or.Else)
#Keep my result reproducible initially tried set.seed(100), try k=5
set.seed(101)
splitPerc = 0.8
trainIndices = sample(1:dim(all_ales)[1],round(splitPerc * dim(all_ales)[1]))
train = all_ales[trainIndices,]
test = all_ales[-trainIndices,]
# try k=5
classifications = knn(train[,3:4],test[,3:4],train$India.Pale.Ale.Or.Else, prob = TRUE, k = 5)
table(classifications,test$India.Pale.Ale.Or.Else)
cm = confusionMatrix(table(classifications,test$India.Pale.Ale.Or.Else))
cm
# explore best possible K value for accuracy
set.seed(101)
iterations = 500
numks = 50
masterAcc = matrix(nrow = iterations, ncol = numks)
  
for(j in 1:iterations)
{
trainIndices = sample(1:dim(all_ales)[1],round(splitPerc * dim(all_ales)[1]))
train = all_ales[trainIndices,]
test = all_ales[-trainIndices,]
for(i in 1:numks)
{
  classifications = knn(train[,3:4],test[,3:4],train$India.Pale.Ale.Or.Else, prob = TRUE, k = i)
  table(classifications,test$India.Pale.Ale.Or.Else)
  CM = confusionMatrix(table(classifications,test$India.Pale.Ale.Or.Else))
  masterAcc[j,i] = CM$overall[1]
}
}
MeanAcc = colMeans(masterAcc)
plot(seq(1,numks,1),MeanAcc, type = "l",main="mean Accuracy vs. different K (number of neighbor used to predict)",
     ylab="Mean Accuracy",xlab="k used")

#Try k=30 after finding it might give the most accuracy
#Keep my result reproducible initially tried set.seed(101)
set.seed(101)
splitPerc = 0.8
trainIndices = sample(1:dim(all_ales)[1],round(splitPerc * dim(all_ales)[1]))
train = all_ales[trainIndices,]
test = all_ales[-trainIndices,]
# try k=5
classifications = knn(train[,3:4],test[,3:4],train$India.Pale.Ale.Or.Else, prob = TRUE, k = 30)
table(classifications,test$India.Pale.Ale.Or.Else)
cm = confusionMatrix(table(classifications,test$India.Pale.Ale.Or.Else))
cm

# explore best possible K value for sensitivity
set.seed(101)
iterations = 500
numks = 50
masterSen = matrix(nrow = iterations, ncol = numks)
  
for(j in 1:iterations)
{
trainIndices = sample(1:dim(all_ales)[1],round(splitPerc * dim(all_ales)[1]))
train = all_ales[trainIndices,]
test = all_ales[-trainIndices,]
for(i in 1:numks)
{
  classifications = knn(train[,3:4],test[,3:4],train$India.Pale.Ale.Or.Else, prob = TRUE, k = i)
  table(classifications,test$India.Pale.Ale.Or.Else)
  CM = confusionMatrix(table(classifications,test$India.Pale.Ale.Or.Else))
  masterSen[j,i] = CM$byClass[1]
}
}
MeanSen = colMeans(masterSen)
plot(seq(1,numks,1),MeanSen, type = "l",main="mean Sensitivity vs. different K (number of neighbor used to predict)",
     ylab="Mean Sensitivity",xlab="k used")

#Try k=3 after finding it might give the most Sensitivity
#Keep my result reproducible initially tried set.seed(101)
set.seed(101)
splitPerc = 0.8
trainIndices = sample(1:dim(all_ales)[1],round(splitPerc * dim(all_ales)[1]))
train = all_ales[trainIndices,]
test = all_ales[-trainIndices,]
# try k=5
classifications = knn(train[,3:4],test[,3:4],train$India.Pale.Ale.Or.Else, prob = TRUE, k = 3)
table(classifications,test$India.Pale.Ale.Or.Else)
cm = confusionMatrix(table(classifications,test$India.Pale.Ale.Or.Else))
cm
#Get an average percentage of Accuracy, Sensitivity, and Specificity of KNN model k=3
set.seed(101)
iterations = 100
numks = 50
masterAcc = matrix(nrow = iterations, ncol = 1)
masterSen = matrix(nrow = iterations, ncol = 1)
masterSpec = matrix(nrow = iterations, ncol = 1)
for(j in 1:iterations)
{
trainIndices = sample(1:dim(all_ales)[1],round(splitPerc * dim(all_ales)[1]))
train = all_ales[trainIndices,]
test = all_ales[-trainIndices,]
classifications = knn(train[,3:4],test[,3:4],train$India.Pale.Ale.Or.Else, prob = TRUE, k = 3)
CM = confusionMatrix(table(classifications,test$India.Pale.Ale.Or.Else))
masterAcc[j,1]=CM$overall[1]
masterSen[j,1]=CM$byClass[1]
masterSpec[j,1]=CM$byClass[2]
}
MeanAcc = colMeans(masterAcc)
MeanSen = colMeans(masterSen)
MeanSpec = colMeans(masterSpec)
MeanAcc
MeanSen
MeanSpec

# Try using Naive Bayes see if it will improve results, split 0.8:
set.seed(102)
splitPerc = 0.8
trainIndices = sample(1:dim(all_ales)[1],round(splitPerc * dim(all_ales)[1]))
train = all_ales[trainIndices,]
test = all_ales[-trainIndices,]
#NB model
model = naiveBayes(train[,3:4],train$India.Pale.Ale.Or.Else)
CM = confusionMatrix(table(predict(model,test[,3:4]),test$India.Pale.Ale.Or.Else))
CM

# Try getting average of Accuracy, Sensitivity and Specificity rate using NB model from 100 random generators
set.seed(101)
splitPerc = .7
iterations = 100
masterAcc = matrix(nrow = iterations, ncol = 1)
masterSen = matrix(nrow = iterations, ncol = 1)
masterSpec = matrix(nrow = iterations, ncol = 1)
for(j in 1:iterations)
{
trainIndices = sample(1:dim(all_ales)[1],round(splitPerc * dim(all_ales)[1]))
train = all_ales[trainIndices,]
test = all_ales[-trainIndices,]
#NB model
model = naiveBayes(train[,3:4],train$India.Pale.Ale.Or.Else)
CM = confusionMatrix(table(predict(model,test[,3:4]),test$India.Pale.Ale.Or.Else))
masterAcc[j,1]=CM$overall[1]
masterSen[j,1]=CM$byClass[1]
masterSpec[j,1]=CM$byClass[2]

}

MeanAcc = colMeans(masterAcc)
MeanSen = colMeans(masterSen)
MeanSpec = colMeans(masterSpec)
MeanAcc
MeanSen
MeanSpec
```






























